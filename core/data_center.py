import sys
import time
from concurrent.futures import ProcessPoolExecutor
from pathlib import Path
from typing import Union, Callable, List, Optional

import numpy as np
import pandas as pd
from tqdm import tqdm
from config import n_jobs
from core.market_essentials import cal_fuquan_price, cal_zdt_price, merge_with_index_data, import_index_data
from core.utils.log_kit import logger

# å®šä¹‰è‚¡ç¥¨æ•°æ®æ‰€éœ€çš„åˆ—
DATA_COLS = [
    "è‚¡ç¥¨ä»£ç ",
    "è‚¡ç¥¨åç§°",
    "äº¤æ˜“æ—¥æœŸ",
    "å¼€ç›˜ä»·",
    "æœ€é«˜ä»·",
    "æœ€ä½ä»·",
    "æ”¶ç›˜ä»·",
    "å‰æ”¶ç›˜ä»·",
    "æˆäº¤é‡",
    "æˆäº¤é¢",
    "æµé€šå¸‚å€¼",
    "æ€»å¸‚å€¼",
    "æ²ªæ·±300æˆåˆ†è‚¡",
    'æ–°ç‰ˆç”³ä¸‡ä¸€çº§è¡Œä¸šåç§°'
]


# ================================================================
# step1_æ•´ç†æ•°æ®.py
# ================================================================
def prepare_data(
    stock_data_path: Path,
    index_data_path: Path,
    runtime_folder: Path,
    rebalance_time_list: List[str],
    boost: bool = True
):
    """
    å‡†å¤‡è‚¡ç¥¨æ•°æ®ï¼Œä¸ä¾èµ–BacktestConfigç±»
    
    å‚æ•°:
    stock_data_path: è‚¡ç¥¨æ•°æ®è·¯å¾„
    index_data_path: æŒ‡æ•°æ•°æ®è·¯å¾„
    runtime_folder: è¿è¡Œæ—¶ç¼“å­˜æ–‡ä»¶å¤¹
    rebalance_time_list: è°ƒä»“æ—¶é—´åˆ—è¡¨
    boost: æ˜¯å¦ä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿ
    """
    logger.info(f"è¯»å–æ•°æ®ä¸­å¿ƒæ•°æ®...")
    start_time = time.time()  # è®°å½•æ•°æ®å‡†å¤‡å¼€å§‹æ—¶é—´
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    runtime_folder.mkdir(parents=True, exist_ok=True)
    
    # 1. è·å–è‚¡ç¥¨ä»£ç åˆ—è¡¨
    stock_code_list = []  # ç”¨äºå­˜å‚¨è‚¡ç¥¨ä»£ç 
    # éå†æ–‡ä»¶å¤¹ä¸‹ï¼Œæ‰€æœ‰csvæ–‡ä»¶
    for filename in stock_data_path.glob("*.csv"):
        # æ’é™¤éšè—æ–‡ä»¶
        if filename.stem.startswith("."):
            continue
        stock_code_list.append(filename.stem)
    stock_code_list = sorted(stock_code_list)
    logger.debug(f"ğŸ“‚ è¯»å–åˆ°è‚¡ç¥¨æ•°é‡ï¼š{len(stock_code_list)}")

    # 2. è¯»å–å¹¶å¤„ç†æŒ‡æ•°æ•°æ®ï¼Œç¡®ä¿è‚¡ç¥¨æ•°æ®ä¸æŒ‡æ•°æ•°æ®çš„æ—¶é—´å¯¹é½
    index_data = import_index_data(index_data_path / "sh000001.csv", ["2007-01-01", None])
    all_candle_data_dict = {}  # ç”¨äºå­˜å‚¨æ‰€æœ‰è‚¡ç¥¨çš„Kçº¿æ•°æ®

    logger.debug(f"ğŸš€ å¤šè¿›ç¨‹å¤„ç†æ•°æ®ï¼Œè¿›ç¨‹æ•°é‡ï¼š{n_jobs}" if boost else "ğŸš² å•è¿›ç¨‹å¤„ç†æ•°æ®")
    if boost:
        with ProcessPoolExecutor(max_workers=n_jobs) as executor:
            futures = []
            for code in stock_code_list:
                file_path = stock_data_path / f"{code}.csv"
                futures.append(executor.submit(prepare_data_by_stock, file_path, index_data, rebalance_time_list))

            for future in tqdm(futures, desc="ğŸ“¦ å¤„ç†æ•°æ®", total=len(futures), mininterval=2, file=sys.stdout):
                df = future.result()
                if not df.empty:
                    code = df["è‚¡ç¥¨ä»£ç "].iloc[0]
                    all_candle_data_dict[code] = df  # ä»…å­˜å‚¨éç©ºæ•°æ®
    else:
        for code in tqdm(
            stock_code_list, desc="ğŸ“¦ å¤„ç†æ•°æ®", total=len(stock_code_list), mininterval=2, file=sys.stdout
        ):
            file_path = stock_data_path / f"{code}.csv"
            df = prepare_data_by_stock(file_path, index_data, rebalance_time_list)
            if not df.empty:
                all_candle_data_dict[code] = df

    # è·å–æ‰€æœ‰è‚¡ç¥¨æ•°æ®çš„æœ€å¤§æ—¥æœŸ
    max_candle_date = max([df["äº¤æ˜“æ—¥æœŸ"].max() for df in all_candle_data_dict.values()])

    # 3. ç¼“å­˜é¢„å¤„ç†åçš„æ•°æ®
    cache_path = runtime_folder / "è‚¡ç¥¨é¢„å¤„ç†æ•°æ®.pkl"
    logger.debug(f"ğŸ“ˆ ä¿å­˜è‚¡ç¥¨é¢„å¤„ç†æ•°æ®: {cache_path}")
    logger.debug(f"ğŸ“… è¡Œæƒ…æ•°æ®æœ€æ–°äº¤æ˜“æ—¥æœŸï¼š{max_candle_date}")
    pd.to_pickle(all_candle_data_dict, cache_path)

    # 4. å‡†å¤‡å¹¶ç¼“å­˜pivoté€è§†è¡¨æ•°æ®ï¼Œç”¨äºåç»­å›æµ‹
    logger.debug("ğŸ“„ ç”Ÿæˆè¡Œæƒ…æ•°æ®é€è§†è¡¨...")
    market_pivot_dict = make_market_pivot(all_candle_data_dict, rebalance_time_list)
    pivot_cache_path = runtime_folder / "å…¨éƒ¨è‚¡ç¥¨è¡Œæƒ…pivot.pkl"
    logger.debug(f"ğŸ—„ï¸ ä¿å­˜è¡Œæƒ…æ•°æ®é€è§†è¡¨: {pivot_cache_path}")
    pd.to_pickle(market_pivot_dict, pivot_cache_path)

    logger.ok(f"æ•°æ®å‡†å¤‡è€—æ—¶ï¼š{(time.time() - start_time):.2f} ç§’")


def prepare_data_by_stock(
    stock_file_path: Union[str, Path], index_data: pd.DataFrame, rebalance_time_list: List[str]
) -> pd.DataFrame:
    """
    å¯¹è‚¡ç¥¨æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼ŒåŒ…æ‹¬åˆå¹¶æŒ‡æ•°æ•°æ®å’Œè®¡ç®—æœªæ¥äº¤æ˜“æ—¥çŠ¶æ€ã€‚

    å‚æ•°:
    stock_file_path (str | Path): è‚¡ç¥¨æ—¥çº¿æ•°æ®çš„è·¯å¾„
    index_data (DataFrame): æŒ‡æ•°æ•°æ®
    rebalance_time_list (List[str]): è°ƒä»“æ—¶é—´åˆ—è¡¨

    è¿”å›:
    df (DataFrame): é¢„å¤„ç†åçš„æ•°æ®
    """
    # è®¡ç®—æ¶¨è·Œå¹…ã€æ¢æ‰‹ç‡ç­‰å…³é”®æŒ‡æ ‡
    df = pd.read_csv(
        stock_file_path, encoding="gbk", skiprows=1, parse_dates=["äº¤æ˜“æ—¥æœŸ"], usecols=DATA_COLS
    )
    pct_change = df["æ”¶ç›˜ä»·"] / df["å‰æ”¶ç›˜ä»·"] - 1
    turnover_rate = df["æˆäº¤é¢"] / df["æµé€šå¸‚å€¼"]
    trading_days = df.index.astype("int") + 1
    avg_price = df["æˆäº¤é¢"] / df["æˆäº¤é‡"]

    # ä¸€æ¬¡æ€§èµ‹å€¼æé«˜æ€§èƒ½
    df = df.assign(æ¶¨è·Œå¹…=pct_change, æ¢æ‰‹ç‡=turnover_rate, ä¸Šå¸‚è‡³ä»Šäº¤æ˜“å¤©æ•°=trading_days, å‡ä»·=avg_price)

    # å¤æƒä»·è®¡ç®—åŠæ¶¨è·Œåœä»·æ ¼è®¡ç®—
    df = cal_fuquan_price(df, fuquan_type="åå¤æƒ")
    df = cal_zdt_price(df)

    # åˆå¹¶è‚¡ç¥¨ä¸æŒ‡æ•°æ•°æ®ï¼Œè¡¥å…¨åœç‰Œæ—¥æœŸç­‰ä¿¡æ¯
    df = merge_with_index_data(df, index_data.copy(), fill_0_list=["æ¢æ‰‹ç‡"])

    # è‚¡ç¥¨é€€å¸‚æ—¶é—´å°äºæŒ‡æ•°å¼€å§‹æ—¶é—´ï¼Œå°±ä¼šå‡ºç°ç©ºå€¼
    if df.empty:
        # å¦‚æœå‡ºç°è¿™ç§æƒ…å†µï¼Œè¿”å›ç©ºçš„DataFrameç”¨äºåç»­æ“ä½œ
        return pd.DataFrame(columns=[*DATA_COLS, *rebalance_time_list])

    # è®¡ç®—å¼€ç›˜ä¹°å…¥æ¶¨è·Œå¹…å’Œæœªæ¥äº¤æ˜“æ—¥çŠ¶æ€
    df = df.assign(
        ä¸‹æ—¥_æ˜¯å¦äº¤æ˜“=df["æ˜¯å¦äº¤æ˜“"].astype("int8").shift(-1),
        ä¸‹æ—¥_ä¸€å­—æ¶¨åœ=df["ä¸€å­—æ¶¨åœ"].astype("int8").shift(-1),
        ä¸‹æ—¥_å¼€ç›˜æ¶¨åœ=df["å¼€ç›˜æ¶¨åœ"].astype("int8").shift(-1),
        ä¸‹æ—¥_æ˜¯å¦ST=df["è‚¡ç¥¨åç§°"].str.contains("ST").astype("int8").shift(-1),
        ä¸‹æ—¥_æ˜¯å¦S=df["è‚¡ç¥¨åç§°"].str.contains("S").astype("int8").shift(-1),
        ä¸‹æ—¥_æ˜¯å¦é€€å¸‚=df["è‚¡ç¥¨åç§°"].str.contains("é€€").astype("int8").shift(-1),
    )

    # å¤„ç†æœ€åä¸€æ ¹Kçº¿çš„æ•°æ®ï¼šæœ€åä¸€æ ¹Kçº¿é»˜è®¤æ²¿ç”¨å‰ä¸€æ—¥çš„æ•°æ®
    state_cols = ["ä¸‹æ—¥_æ˜¯å¦äº¤æ˜“", "ä¸‹æ—¥_æ˜¯å¦ST", "ä¸‹æ—¥_æ˜¯å¦S", "ä¸‹æ—¥_æ˜¯å¦é€€å¸‚"]
    df[state_cols] = df[state_cols].ffill()

    return df



def make_market_pivot(market_dict, rebalance_time_list):
    """
    æ„å»ºå¸‚åœºæ•°æ®çš„pivoté€è§†è¡¨ï¼Œä¾¿äºå›æµ‹è®¡ç®—ã€‚

    å‚æ•°:
    market_dict (dict): è‚¡ç¥¨Kçº¿æ•°æ®å­—å…¸
    rebalance_time_list (list):åˆ†é’Ÿæ•°æ®çš„å­—æ®µåˆ—è¡¨

    è¿”å›:
    dict: åŒ…å«å¼€ç›˜ä»·ã€æ”¶ç›˜ä»·åŠå‰æ”¶ç›˜ä»·çš„é€è§†è¡¨æ•°æ®
    """
    # cols = ["äº¤æ˜“æ—¥æœŸ", "è‚¡ç¥¨ä»£ç ", "å¼€ç›˜ä»·", "æ”¶ç›˜ä»·", "å‰æ”¶ç›˜ä»·", *rebalance_time_list]
    # counts = 3 + len(rebalance_time_list)
    cols = ["äº¤æ˜“æ—¥æœŸ", "è‚¡ç¥¨ä»£ç ", "å¼€ç›˜ä»·", "æ”¶ç›˜ä»·", "å‰æ”¶ç›˜ä»·"]
    counts = 3
    count = 1

    logger.debug("âš—ï¸ åˆæˆæ•´ä½“å¸‚åœºæ•°æ®...")
    df_list = [df[cols].dropna(subset="è‚¡ç¥¨ä»£ç ") for df in market_dict.values()]
    df_all_market = pd.concat(df_list, ignore_index=True)
    col_names = {"å¼€ç›˜ä»·": "open", "æ”¶ç›˜ä»·": "close", "å‰æ”¶ç›˜ä»·": "preclose"}

    markets = {}
    for col in cols[2:]:
        logger.debug(f"[{count}/{counts}] {col}é€è§†è¡¨...")
        df_col = df_all_market.pivot(values=col, index="äº¤æ˜“æ—¥æœŸ", columns="è‚¡ç¥¨ä»£ç ")
        markets[col_names.get(col, col)] = df_col
        count += 1

    return markets

